# Welcome to STA 561 Probabilistic Machine Learning

## Quick references:
- Instructor: Eric Laber, eric.laber@duke.edu, laber-labs.com
- Office hours:  M-W 10-11AM or by appt, location: zoom 
- TA office hours will be set in your lab sections

## Overview 
The goal of this course is introduce the statsitical underpinnings needed to solve a
many modern statistical problems.  Our focus will be on key ideas in prediction and decision 
making. Often we will try to find the simplest version of a problem/algorithm/idea 
that illustrates salient features while leaving more complex nuanced versions to 
homework or to self-study.  On a related note, this course is **not** a catalog 
of machine learning algorithms and all their variants; such a catalog would
immediately be out-of-date as new methods are constantly being introduced (furthemore, 
learning and using new methods becomes dramatically easier if one has strong intuitive and theoretical
understanding of the foundations of statistics/ML.)  While much of our lecture time will be
spent on proofs and derivations, the homework will involve putting these indeas into practice 
with simulation experiments or data anlyses.  

## Pre-requisites 
I will assume that students havea basic understanding of mathematical statistics,
calculus, basic analyses, linear algebgra, and computing.  There are many excellent 
resources online for shoring gaps in these areas.  Coursera, EdX, Udemy, YouTube, etc., are a
great place to start.  While I will do my best to review key ideas, I will take for
granted that students know basic results such as the strong law of large numbers, the
central limit theorem, and matrix decompositions.  I will also be holding several review
sessions throughout the semester to help students prepare for more technical material if
it appears that there is interest.  


## Syllabus (subject to change; roughly one topic per week)
1.  Linear regression review 
2.  Linear regression and regularization and noise addition 
3.  Cross-validation and inference
4.  Post selection inference  
5.  Linear regression and online estimation 
6.  Kernel methods 
7.  Random forests 
8.  Partial linear models 
9.  Active learning (i.e., sequential experimental design)
10.  Large margin classifiers 
11.  Nearest neight methods 
12.  Batch decision problems (one-stage)
13.  Batch decision problems (mult-stage)
14.  Contextual bandits 
15.  Reinforcement learning

## References
We will primarily use slides and the (virtual) whiteboard for lectures.  A list of
references for background and/or further study will be provided with each topic. 
General references that you may find useful include: 
- *Elements of Statistical Learning* Hastie, Tibshirani, and Freedman  [PDF](https://web.stanford.edu/~hastie/ElemStatLearn/)
- *Reinforcement learning* Sutton and Barto [PDF](http://incompleteideas.net/book/the-book.html)
- *Pattern Classification* Duda, Hart, and Stork [Amazon](https://www.amazon.com/Pattern-Classification-Pt-1-Richard-Duda/dp/0471056693/ref=sr_1_1?dchild=1&keywords=duda+and+hart&qid=1608491709&sr=8-1), there are pdfs online from the authors and copies at the library but they ask others not to distribute so not linked here
